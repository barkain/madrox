# Codex IPC Fixes - October 16, 2025

## Executive Summary

Fixed critical bugs preventing Codex instances from using bidirectional messaging (`reply_to_caller` tool) in STDIO transport mode. The root cause was a cascade of initialization and validation issues related to cross-process IPC with `multiprocessing.Manager`.

## Problem Statement

When Codex instances attempted to use the `reply_to_caller` tool, they received errors:
```json
{"success": false, "error": "Instance {id} not found"}
```

This broke bidirectional messaging, forcing fallback to slower polling-based communication.

## Root Causes Identified

### Issue #1: Async Event Loop Ordering Bug
**Location**: `src/orchestrator/tmux_instance_manager.py:72-75` (original)

**Problem**: `_start_manager_health_monitoring()` was called during `__init__`, which invoked `asyncio.create_task()` before the event loop existed.

**Impact Chain**:
1. `RuntimeError: no running event loop` raised silently
2. `shared_state` became None due to initialization failure
3. No `MADROX_MANAGER_*` environment variables passed to Codex
4. STDIO IPC completely broken

**Fix**: Deferred health monitoring startup to `spawn_instance()` with lazy initialization:
```python
# Lazy startup of health monitoring (after event loop is running)
if self.shared_state and not self._health_monitoring_enabled:
    try:
        self._start_manager_health_monitoring()
        self._health_monitoring_enabled = True
        logger.info("Manager health monitoring started (lazy initialization)")
    except Exception as e:
        logger.error(f"Failed to start health monitoring: {e}", exc_info=True)
```

**Files Modified**: `src/orchestrator/tmux_instance_manager.py`

### Issue #2: Instance Validation in HTTP Server
**Location**: `src/orchestrator/instance_manager.py:673` (original)

**Problem**: `handle_reply_to_caller` checked if `instance_id` exists in `self.instances`, but STDIO subprocesses spawn their own empty `InstanceManager`.

**Architecture**:
- HTTP server: Has all instances in `self.instances`
- STDIO subprocess: Spawns new `InstanceManager` with empty `self.instances`, only shares queues via Manager

**Fix**: Skip instance validation when `shared_state_manager` exists:
```python
# CRITICAL FIX: When using shared_state (STDIO transport), STDIO subprocesses
# don't have instances in their local dict - only response queues are shared.
# Skip instance validation and let TmuxInstanceManager handle the reply.
if not self.shared_state_manager and instance_id not in self.instances:
    return {"success": False, "error": f"Instance {instance_id} not found"}
```

**Files Modified**: `src/orchestrator/instance_manager.py`

### Issue #3: Instance Validation in Tmux Manager
**Location**: `src/orchestrator/tmux_instance_manager.py:1545-1547` (original)

**Problem**: Similar validation issue at the Tmux layer - subprocess doesn't have instance metadata locally.

**Fix**: Skip validation for STDIO transport and handle null instance gracefully:
```python
instance = self.instances.get(instance_id)
# CRITICAL FIX: Skip instance validation for STDIO transport
# STDIO subprocesses don't have instances in their local dict - only HTTP server does
if not instance and not self.shared_state:
    return {"success": False, "error": f"Instance {instance_id} not found"}

# Determine the caller (parent instance or coordinator)
# For STDIO transport, parent_id may not be available locally
parent_id = instance.get("parent_instance_id") if instance else None
delivered_to = parent_id if parent_id else "coordinator"
```

**Files Modified**: `src/orchestrator/tmux_instance_manager.py`

### Issue #4: Message Registry Access
**Location**: `src/orchestrator/tmux_instance_manager.py:1557-1565` (original)

**Problem**: STDIO subprocess tried to call `shared_state.update_message_status()`, but the message wasn't in the subprocess's registry.

**Architecture Limitation**: `multiprocessing.Manager.dict()` creates a NEW shared dict for each connection - it doesn't reconnect to existing dicts. Only queues are properly shared.

**Fix**: Handle `KeyError` gracefully since message status updates are non-critical:
```python
if self.shared_state:
    try:
        self.shared_state.update_message_status(
            correlation_id,
            status="replied",
            reply_content=reply_message,
            replied_at=datetime.now().isoformat(),
        )
    except KeyError:
        # STDIO subprocess doesn't have access to parent's message registry
        # This is expected - just log and continue with queueing the reply
        logger.debug(
            f"Message {correlation_id} not in local registry (STDIO subprocess), "
            f"skipping status update"
        )
```

**Files Modified**: `src/orchestrator/tmux_instance_manager.py`

## Technical Architecture

### IPC Components

```
HTTP Server Process (Parent)
├── InstanceManager
│   ├── instances: {...}  # Full instance registry
│   ├── shared_state_manager: SharedStateManager
│   │   ├── manager: multiprocessing.Manager (daemon)
│   │   ├── response_queues: {instance_id: Queue}  # ✅ Shared
│   │   ├── message_registry: DictProxy  # ❌ NOT shared (child gets new dict)
│   │   └── manager_address, authkey  # Passed via env vars
│   └── tmux_manager: TmuxInstanceManager

Codex Instance Process (Child via STDIO)
├── run_orchestrator.py (STDIO mode)
│   ├── Connects to parent's Manager daemon
│   │   └── Uses MADROX_MANAGER_* env vars
│   ├── InstanceManager (NEW instance)
│   │   ├── instances: {}  # ❌ EMPTY (not shared)
│   │   └── shared_state_manager: SharedStateManager
│   │       ├── manager: RemoteManager (connected to parent)
│   │       └── response_queues: {instance_id: Queue}  # ✅ SAME queues as parent
│   └── MCP Server (STDIO)
│       └── Exposes reply_to_caller tool
```

### Data Flow

1. **Instance Spawn**: HTTP server creates Codex instance with `MADROX_MANAGER_*` env vars
2. **STDIO Launch**: Codex subprocess connects to parent's Manager daemon
3. **Queue Creation**: Parent creates response queue for instance (shared via Manager)
4. **Message Send**: Coordinator sends message to Codex via tmux
5. **Tool Call**: Codex calls `reply_to_caller` via MCP
6. **Reply Routing**:
   - STDIO subprocess: `reply_to_caller` → MCP server → `handle_reply_to_caller`
   - Skips instance validation (subprocess has empty dict)
   - Skips message status update (subprocess has empty registry)
   - Puts reply in shared queue (✅ works!)
7. **Reply Delivery**: HTTP server reads reply from shared queue and returns to caller

## Testing

### Test Case: Codex Bidirectional Messaging
```bash
# Spawn Codex instance
codex_id=$(spawn_codex name="test" model="codex-mini-latest")

# Send message with wait_for_response=true
send_to_instance(
    instance_id=$codex_id,
    message="Test reply_to_caller. Please respond using the tool.",
    wait_for_response=true
)
```

**Expected Result**:
```json
{
  "success": true,
  "delivered_to": "coordinator",
  "correlation_id": "...",
  "timestamp": "..."
}
```

### Verification
✅ Codex instance spawns successfully with Manager credentials
✅ `reply_to_caller` returns `{"success": true}`
✅ Response delivered instantly via shared queue
✅ No "Instance not found" errors
✅ No "Message not found in registry" errors (caught and logged)

## Files Modified

1. **src/orchestrator/tmux_instance_manager.py**
   - Lines 66-74: Added lazy health monitoring flags
   - Lines 442-449: Implemented lazy startup in `spawn_instance()`
   - Lines 1545-1554: Fixed instance validation for STDIO
   - Lines 1557-1577: Added KeyError handling for message registry

2. **src/orchestrator/instance_manager.py**
   - Lines 673-677: Fixed instance validation skip logic

## Performance Impact

- **Positive**: Bidirectional messaging now uses instant queue delivery instead of 60s polling fallback
- **Response Time**: <100ms (queue) vs 1-60s (polling)
- **Resource Usage**: Minimal - only adds lazy initialization check

## Future Improvements

1. **Shared Message Registry**: Modify `SharedStateManager` to properly share the message registry dict across processes (requires custom Manager registration)

2. **Connection Pooling**: Reuse Manager connections for multiple Codex instances to reduce overhead

3. **Health Monitoring**: Add metrics for IPC queue depth and message latency

4. **Error Recovery**: Implement automatic reconnection if Manager daemon dies

## Commit Message

```
fix: resolve Codex IPC issues for bidirectional messaging

Fixed four critical bugs preventing Codex instances from using reply_to_caller:

1. Async event loop ordering bug causing SharedStateManager initialization failure
2. Instance validation failing in HTTP server for STDIO subprocess calls
3. Instance validation failing in Tmux manager for STDIO subprocess calls
4. Message registry KeyError when STDIO subprocess lacks local message

Root cause: STDIO subprocesses spawn isolated InstanceManager with empty
instance dict, only response queues are shared via multiprocessing.Manager.

Solution: Skip validation when shared_state exists, handle missing instance
metadata gracefully, and catch KeyError for message registry updates.

Result: Codex instances now successfully use reply_to_caller for instant
bidirectional messaging instead of falling back to polling.

Files modified:
- src/orchestrator/tmux_instance_manager.py
- src/orchestrator/instance_manager.py

Verified: End-to-end testing confirms reply_to_caller returns success=true
and delivers messages instantly via shared queue.
```

## Related Issues

- Previous commit: `fb15585` - Fixed parent_instance_id injection
- Related feature: Bidirectional messaging protocol (Issue #247)
- Architecture: STDIO transport with multiprocessing.Manager IPC

## References

- Python `multiprocessing.Manager` documentation
- FastMCP STDIO transport implementation
- Madrox IPC design document
